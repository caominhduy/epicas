{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import epicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab, we have tried to import and merge cleaned data into Epicas' `StructuredData`. If you have not read it and do not know what `StructuredData` is, please see it [here](https://github.com/caominhduy/epicas/docs/ipynb/1_loading_and_merging.ipynb).\n",
    "\n",
    "In this one, we are going to move on to the next important step of our pipeline: feature engineering. Don't worry, most of these steps are also automated so you should move very quickly to modeling!\n",
    "![feature engineering meme](https://memegenerator.net/img/instances/70504510/all-feature-engineering-and-no-modeling-makes-quincy-a-dull-boy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We are implementing `EpiData` for this purpose. In this lab, let's try to cover everything. \n",
    "\n",
    "First, let's reload StructuredData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu = epicas.StructuredData(\n",
    "        'demo/datasets/covid.xz',\n",
    "        location = 'FIPS',\n",
    "        date = 'date',\n",
    "        incidence = 'confirmed_cases',\n",
    "        )\n",
    "\n",
    "mobility = epicas.StructuredData(\n",
    "        'demo/datasets/mobility.csv.gz',\n",
    "        location = 'FIPS',\n",
    "        date = 'date'\n",
    "        )\n",
    "\n",
    "population = epicas.StructuredData(\n",
    "        'Reichlab_Population.csv',\n",
    "        location = 'location',\n",
    "        usecols = ['location', 'population']\n",
    "        )\n",
    "\n",
    "merged = jhu + mobility + population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's start!\n",
    "\n",
    "### Load `EpiData` from `StructuredData`\n",
    "\n",
    "To do this, we need to specify 2 minimum hyperparameters: `StructuredData` and `y`. `y` is just the name of our target variable (the time-series that we are trying to forecast) from `StructuredData`. However, to make the later part of our pipeline more accurate and efficient, let's also specify `disease`. In this version, Epicas supports these infectious diseases:\n",
    "\n",
    "+ 'influenza'\n",
    "+ 'covid19'\n",
    "+ 'covid19_alpha'\n",
    "+ 'covid19_delta'\n",
    "+ 'sars'\n",
    "+ 'mers'\n",
    "+ 'common_cold'\n",
    "+ 'ebola'\n",
    "+ 'measles'\n",
    "+ 'mump'\n",
    "+ 'hiv'\n",
    "+ 'hantavirus'\n",
    "+ 'polio'\n",
    "+ 'chickenpox'\n",
    "\n",
    "I strongly recommends trying to pick the disease that is closest to your forecasting disease. When in doubt, or none of the previous resembles your disease well, choose from the list below based on transmission type. Again, let's try to pick the closest one we can!\n",
    "\n",
    "+ 'generic' (this is worse scenario if you are unsure)\n",
    "+ 'generic_aerosol'\n",
    "+ 'generic_body_fluid'\n",
    "+ 'generic_fecal_oral'\n",
    "+ 'generic_respiratory' (this includes droplets and aerosol)\n",
    "+ 'generic_respiratory_droplet'\n",
    "\n",
    "Each disease has its own usual incubation periods, transmission type, etc., of which Epicas has taken care. These will have smaller effect on training performance, but they should significantly narrows down the computational cost while feature engineering. In many cases, it also increases our model accuracy! \n",
    "\n",
    "Since the data we have loaded is COVID-19, and this does not specify the variant, so we are going with 'covid19'. We are also trying to forecast incident cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = epicas.EpiData(merged, y='incidence', disease='covid19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!\n",
    "\n",
    "### Imputation\n",
    "\n",
    "Imputation is the process of replacing missing data points with substitutes. Since Epicas is built on top of Pandas, we are expecting similar options.\n",
    "\n",
    "+ 'median': fill missing values with medians (this option is more robust to outliers)\n",
    "\n",
    "+ 'mean': fill missing values with mean values\n",
    "\n",
    "+ 'zero': fill missing values with 0\n",
    "\n",
    "+ 'ffill': fill missing values by propagating last valid observations forward to next\n",
    "\n",
    "+ 'bfill': fill missing values with next available observations\n",
    "\n",
    "By default, if not specified, our default method will be `median`. Let's try..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.imputation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Alternatively, if we want to ffill..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.imputation(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: this technique only uses observations we have to fill the observations we do not have, which a very naive solution. Thus, it can not replace the importance of a good datasets.\n",
    "\n",
    "### Moving Average of Target Time-series\n",
    "\n",
    "Many types of data inevitably fluctuate strongly over time. Some examples are stock price, amount of trade, price of Dogecoin (just kidding), etc. In our use case, epidemiology data is also fluctuating very heavily, especially during the epidemic outbreaks. Taking moving average may smooth out the data, allowing us human read the trends easier, and may improve overall performance of our model.\n",
    "\n",
    "To take MA of target time-series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.target_to_ma(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we just calculate MA values of incidence and replace them in-place. Notice `3` that I specified as first argument, it means \"taking moving averages with window size of 3.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "There are many reasons for data normalization. For example, one of them is that data normalization helps our model be less sensitive to different scales of features. E.g., with population data we are expecting generally large whole numbers while with mobility data we are expecting real values between -1 and 1. A *very naive* model may put more weights on population data, which is very very bad!\n",
    "\n",
    "To normalize features, just simply specify the subset of features! Let's normalize three columns: population, fb_movement_change, fb_stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.normalization(subset=['population', 'fb_movement_change', 'fb_stationary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpiData['location', 'date', 'incidence', 'confirmed_cases_norm', 'fb_movement_change', 'fb_stationary', 'population']\n",
      "\n",
      "         location       date  incidence  confirmed_cases_norm  \\\n",
      "0            1001 2020-02-15   0.000000                   0.0   \n",
      "1            1001 2020-02-16   0.000000                   0.0   \n",
      "2            1001 2020-02-17   0.000000                   0.0   \n",
      "3            1001 2020-02-18   0.000000                   0.0   \n",
      "4            1001 2020-02-19   0.000000                   0.0   \n",
      "...           ...        ...        ...                   ...   \n",
      "1344613     56045 2021-08-02  10.000000                 129.0   \n",
      "1344614     56045 2021-08-03  10.000000                 144.0   \n",
      "1344615     56045 2021-08-04   9.666667                 144.0   \n",
      "1344616     56045 2021-08-05  10.000000                 144.0   \n",
      "1344617     56045 2021-08-06   9.666667                 129.0   \n",
      "\n",
      "         fb_movement_change  fb_stationary  population  \n",
      "0                  0.439024       0.560976    0.005548  \n",
      "1                  0.439024       0.560976    0.005548  \n",
      "2                  0.326829       0.673171    0.005548  \n",
      "3                  0.468293       0.531707    0.005548  \n",
      "4                  0.468293       0.531707    0.005548  \n",
      "...                     ...            ...         ...  \n",
      "1344613            0.243902       0.756098    0.000673  \n",
      "1344614            0.292683       0.707317    0.000673  \n",
      "1344615            0.248780       0.751220    0.000673  \n",
      "1344616            0.292683       0.707317    0.000673  \n",
      "1344617            0.297561       0.702439    0.000673  \n",
      "\n",
      "[1344618 rows x 7 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, it worked! Notice how population is now narrowed to below 1, on par with mobility data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag Reduction\n",
    "\n",
    "Finally! We are getting to the fun part. First, what is lag? Lag is understood as the delay between changes in regressors and changes in actual target variables. For example, intuitively, if mobility data peaked on some specific day, it would be on some day+n where real cases were recorded (considering incubation period, testing time, delay in test reports, etc.)\n",
    "\n",
    "Assume we suspect it will take 21 days from a change in fb_movement_change reflects on incident cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal shift for fb_movement_change: 10\n"
     ]
    }
   ],
   "source": [
    "merged = merged.lag_reduction(subset=['fb_movement_change'], sliding_window=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was quick! As you can see, it does not matter how many days we pick, it always returns the best lag! However, if you pick a greater number, this process should take longer time, since you are giving it more works, so choose wisely...\n",
    "\n",
    "If you do not have a preference, since you already specify a disease type, it will kick it! Let's try to do the same with fb_stationary, except we do not give it a specific range this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal shift for fb_stationary: 12\n"
     ]
    }
   ],
   "source": [
    "merged = merged.lag_reduction(subset=['fb_stationary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and more...\n",
    "\n",
    "We have covered most of important feature engineering techniques that Epicas supports. But that's not everything. I suggest reading our documentation for others, such as:\n",
    "\n",
    "- Feature Selection\n",
    "- Outlier Removal\n",
    "- Cumulative to incidences\n",
    "\n",
    "Thank you for reading and see you next time in modeling with **ARIMA** (**A**uto**r**egressive **I**ntegrated **M**oving **A**verage)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
