{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import epicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello there! Welcome back. \n",
    "\n",
    "Previously, we have loaded, merged, and performed some feature engineering techniques on our data. In this lab, let's try to get some predictions out of it! In this lab, we will use `epicas.Ensemble` to automatically initialize, train, and predict new cases using pre-specified list of models.\n",
    "\n",
    "Let's begin!\n",
    "\n",
    "## Unweighted Average Ensemble Modeling\n",
    "\n",
    "Before we start, let's load data into StructuredData and EpiData (what we have done in the previous labs). If you have not read them, please do check them out here:\n",
    "- [1. Loading and Merging](https://github.com/caominhduy/epicas/docs/ipynb/1_loading_and_merging.ipynb)\n",
    "- [2. Feature Engineering](https://github.com/caominhduy/epicas/docs/ipynb/2_feature_engineering.ipynb)\n",
    "\n",
    "The next code block will be hidden as it is identical to what we have already covered. Feel free to expand it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal shift for fb_movement_change: 10\n",
      "Optimal shift for fb_stationary: 10\n"
     ]
    }
   ],
   "source": [
    "jhu = epicas.StructuredData(\n",
    "        'demo/datasets/covid.xz',\n",
    "        location = 'FIPS',\n",
    "        date = 'date',\n",
    "        incidence = 'confirmed_cases',\n",
    "        )\n",
    "\n",
    "mobility = epicas.StructuredData(\n",
    "        'demo/datasets/mobility.csv.gz',\n",
    "        location = 'FIPS',\n",
    "        date = 'date'\n",
    "        )\n",
    "\n",
    "population = epicas.StructuredData(\n",
    "        'Reichlab_Population.csv',\n",
    "        location = 'location',\n",
    "        usecols = ['location', 'population']\n",
    "        )\n",
    "\n",
    "merged = jhu + mobility + population\n",
    "\n",
    "merged = epicas.EpiData(merged, y='incidence', disease='covid19').imputation().target_to_ma(window=3)\n",
    "\n",
    "merged = merged.normalization(subset=['population', 'fb_movement_change', 'fb_stationary'])\n",
    "\n",
    "merged = merged.lag_reduction(subset=['fb_movement_change', 'fb_stationary'], sliding_window=21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is a beautiful demonstration of AutoML: everything in this one should be automated!\n",
    "\n",
    "**Disclaimer:** In this version of Epicas (0.1.0), we are using the most naive approach to ensemble modeling, which is Unweighted Average Modeling. All predictions from all models we specify will be averaged as our final predictions. We are expecting many more effective methods coming soon (including weighted average, random forest, and model-agnostic and game theoretic explainers like LIME and SHAP). If these sound cool, stay tune!\n",
    "\n",
    "### Date Format\n",
    "\n",
    "To specify the goal end date, you will need to use either `datetime` or a string of date in ISO-8601 format. \n",
    "\n",
    "For example, for September 1, 2021, you can specify as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-09-01'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'2021-09-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2021, 9, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "date(2021,9,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "At the moment, Epicas offers 5 different options:\n",
    "\n",
    "- `attention`: Self-Attention Based Bidirectional LSTM models\n",
    "- `ARIMA`: AutoRegressive Intergrated Moving Average\n",
    "- `LSTM`: Long Short-term Memory\n",
    "- `BiLSTM` or `Bi-LSTM`: Bidirectional LSTM (similar to LSTM, but going in forward and backward directions)\n",
    "- `GRU`: Gated Recurrent Unit. A \"simpler variant\" of LSTM, released in 2014 by Kyunghyun Cho.\n",
    "\n",
    "![GRU meme](https://i.imgflip.com/5powos.jpg)\n",
    "\n",
    "For `Ensemble`, feel free to pick one, some, or all of these! To keep this lab neat, let's try only two models: Attention-based Bi-LSTM and ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention-based Bi-LSTM Fitting:   0%|                                                         | 0/410 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 14, 4)]           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 14, 64)            9472      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 14, 64)            24832     \n",
      "_________________________________________________________________\n",
      "seq_self_attention (SeqSelfA (None, 14, 64)            4097      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 3588      \n",
      "=================================================================\n",
      "Total params: 41,989\n",
      "Trainable params: 41,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention-based Bi-LSTM Fitting: 100%|███████████████████████████████████████████████| 410/410 [05:05<00:00,  1.34it/s]\n",
      "ARIMA Fitting: 100%|███████████████████████████████████████████████████████████████| 2777/2777 [28:11<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  424\n",
      "Model:                ARIMA(12, 1, 0)   Log Likelihood               -1263.651\n",
      "Date:                Fri, 08 Oct 2021   AIC                           2553.302\n",
      "Time:                        04:39:40   BIC                           2605.918\n",
      "Sample:                             0   HQIC                          2574.093\n",
      "                                - 424                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "ar.L1          1.1031      0.037     30.196      0.000       1.031       1.175\n",
      "ar.L2         -0.0202      0.066     -0.309      0.758      -0.149       0.108\n",
      "ar.L3         -0.9259      0.064    -14.517      0.000      -1.051      -0.801\n",
      "ar.L4          1.0191      0.053     19.146      0.000       0.915       1.123\n",
      "ar.L5         -0.1924      0.075     -2.560      0.010      -0.340      -0.045\n",
      "ar.L6         -0.6912      0.077     -8.955      0.000      -0.843      -0.540\n",
      "ar.L7          0.2535      0.074      3.425      0.001       0.108       0.399\n",
      "ar.L8          0.3883      0.073      5.304      0.000       0.245       0.532\n",
      "ar.L9         -0.4070      0.060     -6.753      0.000      -0.525      -0.289\n",
      "ar.L10        -0.0548      0.065     -0.846      0.397      -0.182       0.072\n",
      "ar.L11         0.4421      0.050      8.768      0.000       0.343       0.541\n",
      "ar.L12        -0.3847      0.027    -14.193      0.000      -0.438      -0.332\n",
      "sigma2        22.6925      0.528     42.989      0.000      21.658      23.727\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.17   Jarque-Bera (JB):              9599.21\n",
      "Prob(Q):                              0.68   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):              16.67   Skew:                             1.78\n",
      "Prob(H) (two-sided):                  0.00   Kurtosis:                        26.06\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ARIMA Predicting: 100%|███████████████████████████████████████████████████████████| 2777/2777 [00:11<00:00, 235.30it/s]\n"
     ]
    }
   ],
   "source": [
    "ensembled = epicas.Ensemble(merged, ['attention', 'ARIMA'], '2021-09-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if ARIMA seems being stuck at 0%, it is not. Before training the network, Epicas needs to run a few \\[Augmented Dickey-Fuller\\] and some step-wise fitting tests on the first batch to find the best hyperparameters (p,d) to feed into other batches. Be patient, the rest should be done pretty quickly...\n",
    "\n",
    "**Note:** if you wonder why we have 410 batches to train for LSTM while ARIMA has 2777. Basically, for ARIMA, there is a different model for every locations, so we have 2777 high-quality locations to train on. Meanwhile, for LSTM, in each batch, the model runs on all locations, but we are looking at stepsize of 14 (14-day lookback for 1-day future predictions), so there are 410 different batches to re-fit models based on the length of our train data. If you do not understand what this means, don't worry!\n",
    "\n",
    "### Output\n",
    "\n",
    "Great! That was so easy, wasn't it? Finally, let's see the outputs of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        location       date  incidence  incidence_preds\n",
      "0         1001.0 2020-02-25        0.0              NaN\n",
      "1         1001.0 2020-02-26        0.0              NaN\n",
      "2         1001.0 2020-02-27        0.0              NaN\n",
      "3         1001.0 2020-02-28        0.0              NaN\n",
      "4         1001.0 2020-02-29        0.0              NaN\n",
      "...          ...        ...        ...              ...\n",
      "1541230  56045.0 2021-08-28        NaN        14.313992\n",
      "1541231  56045.0 2021-08-29        NaN        14.342849\n",
      "1541232  56045.0 2021-08-30        NaN        14.371654\n",
      "1541233  56045.0 2021-08-31        NaN        14.399930\n",
      "1541234  56045.0 2021-09-01        NaN        14.428529\n",
      "\n",
      "[1541235 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(ensembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! As you can see, although we trained on two models, there is only one output column (incidence_preds) because the `Ensemble` has taken care of them.\n",
    "\n",
    "Next, let's try to print this model more nicely where the predicted values are intergrated with our incident cases..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>incidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001.0</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001.0</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001.0</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001.0</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001.0</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541230</th>\n",
       "      <td>56045.0</td>\n",
       "      <td>2021-08-28</td>\n",
       "      <td>14.313992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541231</th>\n",
       "      <td>56045.0</td>\n",
       "      <td>2021-08-29</td>\n",
       "      <td>14.342849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541232</th>\n",
       "      <td>56045.0</td>\n",
       "      <td>2021-08-30</td>\n",
       "      <td>14.371654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541233</th>\n",
       "      <td>56045.0</td>\n",
       "      <td>2021-08-31</td>\n",
       "      <td>14.399930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541234</th>\n",
       "      <td>56045.0</td>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>14.428529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1541235 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        location       date  incidence\n",
       "0         1001.0 2020-02-25   0.000000\n",
       "1         1001.0 2020-02-26   0.000000\n",
       "2         1001.0 2020-02-27   0.000000\n",
       "3         1001.0 2020-02-28   0.000000\n",
       "4         1001.0 2020-02-29   0.000000\n",
       "...          ...        ...        ...\n",
       "1541230  56045.0 2021-08-28  14.313992\n",
       "1541231  56045.0 2021-08-29  14.342849\n",
       "1541232  56045.0 2021-08-30  14.371654\n",
       "1541233  56045.0 2021-08-31  14.399930\n",
       "1541234  56045.0 2021-09-01  14.428529\n",
       "\n",
       "[1541235 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensembled.get_predict(in_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! The argument `in_sample` means \"Do you want to include the ground-truth data in the final output or not?\"\n",
    "\n",
    "You can also save this output into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembled.save_predict('first_try.csv', in_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "- ARIMA is a statistical model, it may take much longer time to train and it should require different models for different time-series for best performance (due to its natural inability to \"train_on_batch\").\n",
    "\n",
    "- RNNs models such as LSTM, GRU are usually faster to train, convenient enough to have one model for every location, easy to be saved and reloaded for forecasting, refitting with new data as incremental learning, or applying to similar time-series as transfer learning. However, they may be more susceptible to high variance (overfitting). We are also testing an automatic hypertuner to mitigate this.\n",
    "\n",
    "- In many cases, ARIMA may outperform ML models in forecasting time-series: Mbah, T.J., Ye, H., Zhang, J. et al. Using LSTM and ARIMA to Simulate and Predict Limestone Price Variations. Mining, Metallurgy & Exploration 38, 913–926 (2021). https://doi.org/10.1007/s42461-020-00362-y. Choose your models wisely based on your computational power, use case, and comfort!\n",
    "\n",
    "- Also do not forget to check our documentations for more advanced applications: train, export, load individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
